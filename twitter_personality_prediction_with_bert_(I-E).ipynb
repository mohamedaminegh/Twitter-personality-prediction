{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"twitter_personality_classification_with_bert_(I-E).ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"GXUaAuMdsWIq","outputId":"fca9cc5c-ba2b-4c02-91a9-c2bdbcccd655","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1587572433499,"user_tz":-60,"elapsed":7557,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import re\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.corpus import stopwords \n","from nltk import word_tokenize\n","from keras.models import Model\n","from keras.layers import Dense, Input, Dropout, LSTM, Activation\n","from keras.layers.embeddings import Embedding\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical\n","import tensorflow as tf\n","import random, math"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"X2z5HXPzpT9Y","colab_type":"text"},"source":["Before you can go and use the BERT text representation, you need to install BERT for TensorFlow 2.0. Execute the following pip commands on your terminal to install BERT for TensorFlow 2.0."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GHSQzdp9sm2q","outputId":"ff96e4da-7928-4964-9cc0-f049557b0dbe","colab":{"base_uri":"https://localhost:8080/","height":490},"executionInfo":{"status":"ok","timestamp":1587572487950,"user_tz":-60,"elapsed":50435,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["!pip install bert-for-tf2\n","!pip install sentencepiece"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting bert-for-tf2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/5c/6439134ecd17b33fe0396fb0b7d6ce3c5a120c42a4516ba0e9a2d6e43b25/bert-for-tf2-0.14.4.tar.gz (40kB)\n","\r\u001b[K     |████████                        | 10kB 26.3MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 20kB 6.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 30kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 40kB 4.8MB/s \n","\u001b[?25hCollecting py-params>=0.9.6\n","  Downloading https://files.pythonhosted.org/packages/a4/bf/c1c70d5315a8677310ea10a41cfc41c5970d9b37c31f9c90d4ab98021fd1/py-params-0.9.7.tar.gz\n","Collecting params-flow>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/ac/0d/615c0d4aea541b4f47c761263809a02e160e7a2babd175f0ddd804776cf4/params-flow-0.8.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.18.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.38.0)\n","Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n","  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.4-cp36-none-any.whl size=30114 sha256=e47097e8a5f7e56db6d45a7b3cc37405b1d7392b44ee114e1da07f9f7c647cea\n","  Stored in directory: /root/.cache/pip/wheels/cf/3f/4d/79d7735015a5f523648df90d871ce8e89a7df8185f7703eeab\n","  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for py-params: filename=py_params-0.9.7-cp36-none-any.whl size=7302 sha256=f95fc1e6bd2fb4c525834bae95a5928687a224b8c9be69518a9163642666adb1\n","  Stored in directory: /root/.cache/pip/wheels/67/f5/19/b461849a50aefdf4bab47c4756596e82ee2118b8278e5a1980\n","  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for params-flow: filename=params_flow-0.8.0-cp36-none-any.whl size=15999 sha256=c63481fcca9d87cbe0554ead568afaee26c7125cc6285f68aa3d694bf1d355e1\n","  Stored in directory: /root/.cache/pip/wheels/88/41/05/1a9955d1d01575bbd58aab76e22f8c7eeabba905d551576f43\n","Successfully built bert-for-tf2 py-params params-flow\n","Installing collected packages: py-params, params-flow, bert-for-tf2\n","Successfully installed bert-for-tf2-0.14.4 params-flow-0.8.0 py-params-0.9.7\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 8.9MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.85\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qXxVQyF9pT9d","colab_type":"text"},"source":["Next, you need to make sure that you are running TensorFlow 2.0. Therefore, to make sure that you are running your script via TensorFlow 2.0, execute the following script:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GtkEWFhrs21p","colab":{}},"source":["try:\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass\n","import tensorflow as tf\n","\n","import tensorflow_hub as hub\n","\n","from tensorflow.keras import layers\n","import bert"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RNCsuqUgpT9i","colab_type":"text"},"source":["In the above script, in addition to TensorFlow 2.0, we also import tensorflow_hub, which basically is a place where you can find all the prebuilt and pretrained models developed in TensorFlow. We will be importing and using a built-in BERT model from TF hub."]},{"cell_type":"markdown","metadata":{"id":"HgNxlydwpT9j","colab_type":"text"},"source":["### **Importing and Preprocessing the Dataset**\n","The following script imports the dataset using the read_csv() method of the Pandas dataframe. The script also prints the shape of the dataset."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"40c45wOjtEAj","outputId":"506a043f-9add-4ef4-94f7-c59b31cd4155","colab":{"base_uri":"https://localhost:8080/","height":134},"executionInfo":{"status":"ok","timestamp":1587572738911,"user_tz":-60,"elapsed":1401,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["data = pd.read_csv(\"dataset.csv\")\n","\n","print(data.head())\n","\n","data.isnull().values.any()\n","\n","data.shape"],"execution_count":4,"outputs":[{"output_type":"stream","text":["   type                                              posts\n","0  INFJ  'http://www.youtube.com/watch?v=qsXHcwe3krw|||...\n","1  ENTP  'I'm finding the lack of me in these posts ver...\n","2  INTP  'Good one  _____   https://www.youtube.com/wat...\n","3  INTJ  'Dear INTP,   I enjoyed our conversation the o...\n","4  ENTJ  'You're fired.|||That's another silly misconce...\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(8675, 2)"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pmM6MFc3tHlq","outputId":"dcf710cd-77d9-4ebc-9faf-0b1f5bd31e4c","colab":{"base_uri":"https://localhost:8080/","height":205},"executionInfo":{"status":"ok","timestamp":1587572745638,"user_tz":-60,"elapsed":5994,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["!pip install emoji\n","import emoji\n","\n","def char_is_emoji(character):\n","    return character in emoji.UNICODE_EMOJI\n","\n","def text_has_emoji(text):\n","    return(bool(emoji.get_emoji_regexp().search(text)))\n","\n","def returnEmojis(text):\n","    listEmojis=[]\n","    for c in text:\n","        if (char_is_emoji(c)):\n","            listEmojis.append(c)\n","    return(listEmojis)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Collecting emoji\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/8d/521be7f0091fe0f2ae690cc044faf43e3445e0ff33c574eae752dd7e39fa/emoji-0.5.4.tar.gz (43kB)\n","\r\u001b[K     |███████▌                        | 10kB 23.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 5.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 30kB 7.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 40kB 7.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n","\u001b[?25hBuilding wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.5.4-cp36-none-any.whl size=42176 sha256=dd3e6e3f8e8df102b4469e3362722ee210bfa5cea448ef12b5e7379519f1a083\n","  Stored in directory: /root/.cache/pip/wheels/2a/a9/0a/4f8e8cce8074232aba240caca3fade315bb49fac68808d1a9c\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.5.4\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A8D5CqSopT9s","colab_type":"text"},"source":["Now we are going to extract some features from the posts and add them to the dataset. For example we will use the number of words used in the user posts, the number of emojis ..."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-zLHIz8GtJti","outputId":"49690e97-828e-44c1-919d-c5e06891fa0c","colab":{"base_uri":"https://localhost:8080/","height":168},"executionInfo":{"status":"ok","timestamp":1587572790548,"user_tz":-60,"elapsed":9098,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["data['words_per_comment'] = data['posts'].apply(lambda x: len(x.split())/50)\n","data['question_per_comment'] = data['posts'].apply(lambda x: x.count('?')/50)\n","data['excl_per_comment'] = data['posts'].apply(lambda x: x.count('!')/50)\n","data['ellipsis_per_comment'] = data['posts'].apply(lambda x: x.count('...')/50)\n","data['@_per_comment'] = data['posts'].apply(lambda x: x.count('@')/50)\n","data['#_per_comment'] = data['posts'].apply(lambda x: x.count('#')/50)\n","data['emojis_per_comment'] = data['posts'].apply(lambda x: (len(returnEmojis(x)))/50)\n","print(data.head())\n","data.shape"],"execution_count":7,"outputs":[{"output_type":"stream","text":["   type  ... emojis_per_comment\n","0  INFJ  ...                0.0\n","1  ENTP  ...                0.0\n","2  INTP  ...                0.0\n","3  INTJ  ...                0.0\n","4  ENTJ  ...                0.0\n","\n","[5 rows x 9 columns]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(8675, 9)"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"wkMJ4P7OtNAh","outputId":"b9d8f96f-71a7-47c9-b942-c30518ac3cbd","colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"status":"ok","timestamp":1587572790551,"user_tz":-60,"elapsed":7916,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["full_Pers_list = {'INFP' :0 ,'INTJ' :1 ,'INFJ' :2, 'INTP' :3 ,'ENFP' :4 ,'ENTJ' :5, 'ENTP' :6 ,'ENFJ' :7, 'ISFJ' :8 ,'ISFP' :9 ,'ISTJ' :10 ,'ISTP' :11 ,'ESFJ' :12,'ESFP' :13 ,'ESTJ' :14 ,'ESTP' :15}\n","def type_to_16(typeList):\n","    labels=[]\n","    for t in typeList:\n","        labels.append(full_Pers_list[t])\n","    return(labels)\n","\n","def type16_to_vector_label(data):\n","    for i,label in enumerate(data):\n","        translation=np.zeros((16))\n","        translation[int(label)]=1\n","        data[i]=translation\n","        \n","labels=type_to_16(data[\"type\"])\n","data[\"labels\"]=labels\n","data.tail()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>posts</th>\n","      <th>words_per_comment</th>\n","      <th>question_per_comment</th>\n","      <th>excl_per_comment</th>\n","      <th>ellipsis_per_comment</th>\n","      <th>@_per_comment</th>\n","      <th>#_per_comment</th>\n","      <th>emojis_per_comment</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8670</th>\n","      <td>ISFP</td>\n","      <td>'https://www.youtube.com/watch?v=t8edHB_h908||...</td>\n","      <td>15.92</td>\n","      <td>0.18</td>\n","      <td>0.12</td>\n","      <td>0.14</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>9</td>\n","    </tr>\n","    <tr>\n","      <th>8671</th>\n","      <td>ENFP</td>\n","      <td>'So...if this thread already exists someplace ...</td>\n","      <td>26.18</td>\n","      <td>0.20</td>\n","      <td>0.66</td>\n","      <td>0.82</td>\n","      <td>0.04</td>\n","      <td>0.02</td>\n","      <td>0.0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>8672</th>\n","      <td>INTP</td>\n","      <td>'So many questions when i do these things.  I ...</td>\n","      <td>18.96</td>\n","      <td>0.18</td>\n","      <td>0.02</td>\n","      <td>0.38</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>8673</th>\n","      <td>INFP</td>\n","      <td>'I am very conflicted right now when it comes ...</td>\n","      <td>34.10</td>\n","      <td>0.18</td>\n","      <td>0.06</td>\n","      <td>0.94</td>\n","      <td>0.00</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>8674</th>\n","      <td>INFP</td>\n","      <td>'It has been too long since I have been on per...</td>\n","      <td>27.22</td>\n","      <td>0.12</td>\n","      <td>0.10</td>\n","      <td>0.48</td>\n","      <td>0.02</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      type  ... labels\n","8670  ISFP  ...      9\n","8671  ENFP  ...      4\n","8672  INTP  ...      3\n","8673  INFP  ...      0\n","8674  INFP  ...      0\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4aqLSl9qzBvg","outputId":"a31d1577-d615-4406-d9ca-aa14e62732c1","colab":{"base_uri":"https://localhost:8080/","height":299},"executionInfo":{"status":"ok","timestamp":1587572794480,"user_tz":-60,"elapsed":1151,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["data.head()"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>posts</th>\n","      <th>words_per_comment</th>\n","      <th>question_per_comment</th>\n","      <th>excl_per_comment</th>\n","      <th>ellipsis_per_comment</th>\n","      <th>@_per_comment</th>\n","      <th>#_per_comment</th>\n","      <th>emojis_per_comment</th>\n","      <th>labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>INFJ</td>\n","      <td>'http://www.youtube.com/watch?v=qsXHcwe3krw|||...</td>\n","      <td>11.12</td>\n","      <td>0.36</td>\n","      <td>0.06</td>\n","      <td>0.30</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ENTP</td>\n","      <td>'I'm finding the lack of me in these posts ver...</td>\n","      <td>23.40</td>\n","      <td>0.10</td>\n","      <td>0.00</td>\n","      <td>0.38</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>INTP</td>\n","      <td>'Good one  _____   https://www.youtube.com/wat...</td>\n","      <td>16.72</td>\n","      <td>0.24</td>\n","      <td>0.08</td>\n","      <td>0.26</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>INTJ</td>\n","      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n","      <td>21.28</td>\n","      <td>0.22</td>\n","      <td>0.06</td>\n","      <td>0.52</td>\n","      <td>0.00</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ENTJ</td>\n","      <td>'You're fired.|||That's another silly misconce...</td>\n","      <td>19.34</td>\n","      <td>0.20</td>\n","      <td>0.02</td>\n","      <td>0.42</td>\n","      <td>0.04</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   type  ... labels\n","0  INFJ  ...      2\n","1  ENTP  ...      6\n","2  INTP  ...      3\n","3  INTJ  ...      1\n","4  ENTJ  ...      5\n","\n","[5 rows x 10 columns]"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VlvswzYBtQ3C","colab":{}},"source":["b_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\n","b_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n","\n","def translate_personality(personality):\n","    # transform mbti to binary vector\n","    \n","    return [b_Pers[l] for l in personality]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_RycZ2yztS5Z","outputId":"ef5ba506-3d70-4d8e-ca67-5d6027b13da1","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1587572801234,"user_tz":-60,"elapsed":1679,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","# We want to remove these from the posts\n","unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n","       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n","  \n","unique_type_list = [x.lower() for x in unique_type_list]\n","\n","\n","# Lemmatize\n","stemmer = PorterStemmer()\n","lemmatiser = WordNetLemmatizer()\n","\n","# Cache the stop words for speed \n","cachedStopWords = stopwords.words(\"english\")\n","\n","def pre_process_data(data, remove_stop_words=True, remove_mbti_profiles=True):\n","\n","    list_personality = []\n","    list_posts = []\n","    len_data = len(data)\n","    i=0\n","    \n","    for row in data.iterrows():\n","        i+=1\n","        if (i % 500 == 0 or i == 1 or i == len_data):\n","            print(\"%s of %s rows\" % (i, len_data))\n","\n","        ##### Remove and clean comments using regular expressions\n","        posts = row[1].posts\n","        temp = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n","        temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n","        temp = re.sub(' +', ' ', temp).lower()\n","        if remove_stop_words:\n","            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in cachedStopWords])\n","        else:\n","            temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n","            \n","        if remove_mbti_profiles:\n","            for t in unique_type_list:\n","                temp = temp.replace(t,\"\")\n","        ##### Remove single-charactered words that remain from the cleaning\n","        words=temp.split()\n","        finalTemp=\"\"\n","        for word in words:\n","            if(len(word)>1):\n","                finalTemp=finalTemp+\" \"+word\n","        type_labelized = translate_personality(row[1].type)\n","        list_personality.append(type_labelized)\n","        list_posts.append(finalTemp)\n","\n","    list_posts = np.array(list_posts)\n","    list_personality = np.array(list_personality)\n","    return list_posts, list_personality"],"execution_count":11,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"J6-t1eQktVPi","outputId":"1464b016-3747-46bd-d081-34df6a15397c","colab":{"base_uri":"https://localhost:8080/","height":336},"executionInfo":{"status":"ok","timestamp":1587572847413,"user_tz":-60,"elapsed":42930,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["list_posts, list_personality  = pre_process_data(data, remove_stop_words=True)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["1 of 8675 rows\n","500 of 8675 rows\n","1000 of 8675 rows\n","1500 of 8675 rows\n","2000 of 8675 rows\n","2500 of 8675 rows\n","3000 of 8675 rows\n","3500 of 8675 rows\n","4000 of 8675 rows\n","4500 of 8675 rows\n","5000 of 8675 rows\n","5500 of 8675 rows\n","6000 of 8675 rows\n","6500 of 8675 rows\n","7000 of 8675 rows\n","7500 of 8675 rows\n","8000 of 8675 rows\n","8500 of 8675 rows\n","8675 of 8675 rows\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nCZmN_YntW47","outputId":"0e1a647b-f6ba-4e12-e57b-3a325b9e265c","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1587572848572,"user_tz":-60,"elapsed":723,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["print(list_posts[1])\n","print(list_personality[1])\n","print(data[\"labels\"][1])"],"execution_count":13,"outputs":[{"output_type":"stream","text":[" finding lack post alarming sex boring position often example girlfriend currently environment creatively use cowgirl missionary enough giving new meaning game theory hello grin take converse flirting acknowledge presence return word smooth wordplay cheeky grin lack balance hand eye coordination real iq test score internet iq test funny score higher like former response thread mention believe iq test banish know vanish site year half return find people still commenting post liking idea thought know think thing sometimes go old sherlock holmes quote perhaps man special knowledge special power like rather encourages seek complex cheshirewolf tumblr com post really never thought real function judge use use ne ti dominates fe emotion rarely si also use ni due strength know though ingenious saying really want try see happens playing first person shooter back drive around want see look rock paper one best make lol guy lucky really high tumblr system hear new first person shooter game rocking hell soundtrack auto sound equipment shake heaven managed put couple way connected thing ne ne dominates aware environment se dominates example shawn spencer patrick jane well charlie first admit get jealous like chalk heart mixed dominate like noticed like known upload clip mic away mouth hear anything ninja assassin style splatter tik tok really great song long mental block singer love beat make bounce drop io swck mic really close mouth smokin ace assassin ball playing background sociable extrovert extrovert sociable sherlock movie normally played extj book said movie looked good except called sherlock holmes oh never fear kissing guy kiss animal nothing vanish personal taste liking guy kissed know one sound pretty much like area going right trying figure way want take life want many thing biggest problem know operating impression female never looked boxy okay help gay friend time one developed little crush get red described living worst nightmare trapped one place one one around dull wood serial killer would perfect place sadly tbh biased sound like shadowed think maybe hurt turned tell typical trait left check list sorry seems came bad time already reached quota however female like female make deal kick one antp leaning toward easy identify also imagine interrogation would go little bit like jack except mechanical rigging shock treatment equipment abandoned building old car batty jumper compliment trust psychopathic except emoticon weird one like laughing get hurt people running lawn mower like theme live know heart usual leave thing end mean time time work thing work mine mbp pleasure meet damn need trust instinct would closer going say exfp leaning toward way responded friend even gay lesbian one always come advice bow master great able build building duck duck duck shotgun never hard sad losing someone like knew right give big pat back awesome always correct oh tell stupid know play make laugh going take neuropsychology psychologist nightowl wake pm stay awake till personal opinion backed theory would suggest socially difficult socially indifferent also use social situation need arises personal stock desktop downloaded random stock site stock photobuckets tell open photoshop glad like static thanks made friend several hour work constructed every line static get avatar later one fellow teammate psychologist keep around long enough diagnosis like toy diagnosis psychologist friend friend tell\n","[1 0 1 1]\n","6\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6WseLz2IpT-L","colab_type":"text"},"source":["Our dataset contains ten columns, as can be verified from the following script:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oqhp9HC6tcjW","outputId":"b4de7c97-0aa0-4861-9289-c2e2f1178f16","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1587572853289,"user_tz":-60,"elapsed":1143,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["print(data.columns.values)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["['type' 'posts' 'words_per_comment' 'question_per_comment'\n"," 'excl_per_comment' 'ellipsis_per_comment' '@_per_comment' '#_per_comment'\n"," 'emojis_per_comment' 'labels']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-jwafQyqtgEi","outputId":"621ac838-28dc-4d12-f552-adeb5655270a","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1587572873245,"user_tz":-60,"elapsed":1109,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["Y = list_personality[:,1]\n","print(Y)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["[0 0 0 ... 0 0 0]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HbioHqZ3tlOR","outputId":"c035377b-bb91-41c4-97f5-dfc6476423bf","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1587572874217,"user_tz":-60,"elapsed":917,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["print(list_posts[10])"],"execution_count":16,"outputs":[{"output_type":"stream","text":[" one time parent fighting dad affair dad pushed mom fall broke finger pointed gun made get knee beg life gonna talk piece shit dad alcoholic kind serious mental problem come complying irs word law apply omg woman center lived run catholic charity fat bully program manager took upon change policy tenant forced attend christmas party work calling committed vacation day ever kundalini mystic oh get paid either one destined thing art teacher high school stack art school catalog saw one school ended going immediately knew one without research like communication design nope much execution mystic got degree one best school world field actually career unsustainable engaged inferior function directly case even absolutely know tf think day age familiar employment planet living ever pas kindergarten somebody actually employ issue taking responsibility sexual response called self discipline start mind internalize sexuality mercy oh think also say imply say make sense always healthy allowed develop naturally people something character inferior function immersion put danger pumping exciting danger stress chemical body going grain psychological kind sound believable quote guy stuff like man easy know last year spent month hauling tale woe around craigslist rant know like put hand pant like getting beaten chain since guy immune physicality need much stimulation possible even register alive getting beaten chain get god oriented crisis though first thought grown inferior function living false identity life story parallel closely happened sorry fellow care anything flesh shared body might miss guess long shot physical life ballz told gf parasite dumped threatened punch mouth oh another thing ever sex te person te fe sex go te person actually say gonna make face reason te people call everything unstable sensitive unstable one handle little fe brow scrunch wow grow case manager also probably treated like shit whole life te people treated like shit fe facial expression cold controlled person capricorn moon square eternity time wen nefer know got defragmented osiris osiris underworld pluto shamanism soul retrieval exchange look like become physical le spiritual always le every time meet le le lead yikes mother thought ni stuff scary devil insane abandoned ni age learned hate effort stay good grace avoid wrath although pluto transit whole point get eaten talking relationship se daily basis crisis time thing using inferior mirror like gazing scrying mirror otherwise get eaten imo proper ni se dynamic oh dear tree stronger weaker root never one root healthy one anyway may find balance hug quote problem identification dirty people also remind people know disempowered sense vitality zombie moth attracted source light smoke made lot sacrifice ni think trade talking se dom meet ni dom thing sort interaction could generally characterized mutual relief burden least easiest identify woman take smoke noticed trade going people encounter whether must decode symbol impoverished think try live normal life like criminal run eventually enough inferior se experience criminal surrounded cop everything outside trap everything physical world trap get job something bad happen take walk something bad happen like ok alien ghost living inside left age negative change due constant trauma cuz stuck inferior function literally wow obviously made mind advice wanted get throwing hissy fit cartoon thread gross even comparing even know situation appalled invalidating response people preach gratitude either life far easy live denial daria think cap moon thing type thing person want sex horrific hate physical sex fb profile definitely bad like family people went high school fall category prefer stranger worst dillard sale associate first month reported much theft people fired store manager thanked ops manager told stop reporting lose well tell happens go wrong college party dorm room dorm building guy liked cornered bathroom kissed ha fair younger brother entire top floor house bedroom living area full bath free even though make year already talked elsewhere like saying much possible make people lose mind shower month period shave anywhere stay turf leave mbti sj nothing complain life society structured make sure easy need tool like personality somebody mentioned blaming personality type lack success something helped know limit type stop wanting delusional thing like pop star used get really annoyed surrounded people turned everything said meme know like genuine complaint person invalidates saying think sustainable solution value spirit world physical invisible support\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DRqIIAYPtm-a","outputId":"86fca8c0-32d6-4781-bfb9-8ae3af63674d","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1587572875503,"user_tz":-60,"elapsed":876,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["print(Y[10])"],"execution_count":17,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nk9Kj6f4pT-b","colab_type":"text"},"source":["The output 0 confirms that this is an Introvert. We have now preprocessed our data and we are now ready to create BERT representations from our text data."]},{"cell_type":"markdown","metadata":{"id":"AHVK9EImpT-c","colab_type":"text"},"source":["### ***Creating a BERT Tokenizer***\n","In order to use BERT text embeddings as input to train text classification model, we need to tokenize our posts. Tokenization refers to dividing a sentence into individual words. To tokenize our text, we will be using the BERT tokenizer. Look at the following script:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5hF3CMfjtoDi","colab":{}},"source":["BertTokenizer = bert.bert_tokenization.FullTokenizer\n","bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n","                            trainable=False)\n","vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n","tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FZYceTtApT-g","colab_type":"text"},"source":["In the script above we first create an object of the FullTokenizer class from the bert.bert_tokenization module. Next, we create a BERT embedding layer by importing the BERT model from hub.KerasLayer. The trainable parameter is set to False, which means that we will not be training the BERT embedding. In the next line, we create a BERT vocabulary file in the form a numpy array. We then set the text to lowercase and finally we pass our vocabulary_file and to_lower_case variables to the BertTokenizer object.\n","\n","\n","Let's now see if our BERT tokenizer is actually working. To do so, we will tokenize a random sentence, as shown below:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nwh2UNSitqQK","outputId":"76220dac-2c3e-463e-c75f-d3ecafb94d24","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1587572914122,"user_tz":-60,"elapsed":1066,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["tokenizer.tokenize(\"This is a personality classifier\")"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['this', 'is', 'a', 'personality', 'class', '##ifier']"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"vEzNq9iTpT-k","colab_type":"text"},"source":["You can see that the text has been successfully tokenized. You can also get the ids of the tokens using the convert_tokens_to_ids() of the tokenizer object. Look at the following script:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ckUbB9a6ts0a","outputId":"ae623e0f-bb8f-4c6b-dbb6-ba9e9e496c9c","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1587572917245,"user_tz":-60,"elapsed":1050,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"This is a personality classifier\"))"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[2023, 2003, 1037, 6180, 2465, 18095]"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"hnr9R7MGpT-n","colab_type":"text"},"source":["Now will define a function that accepts a text and returns the ids of the tokenized words in the text. Execute the following script:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0AjMwVzmtt3N","colab":{}},"source":["def tokenize_text(text):\n","    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mLtb33qZpT-r","colab_type":"text"},"source":["And execute the following script to actually tokenize all the posts in the input dataset:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Xck8oJE0t2rB","colab":{}},"source":["tokenized_posts = [tokenize_text(post) for post in list_posts]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"XEjDLGlPt7Pp","outputId":"99d3a1ae-33cc-472e-b372-9e38281f63aa","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1587572999882,"user_tz":-60,"elapsed":76855,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["print(tokenized_posts[0])"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[2617, 2998, 13013, 2121, 2327, 2702, 2377, 26418, 2166, 5278, 3325, 2166, 9377, 2651, 2089, 2566, 2278, 3325, 10047, 16862, 2063, 2197, 2518, 2767, 6866, 9130, 16873, 5920, 2279, 2154, 2717, 3521, 7592, 3374, 2963, 12893, 3019, 3276, 15401, 2051, 2296, 2617, 4598, 3046, 3275, 2524, 2051, 2051, 3930, 6160, 4933, 2208, 2275, 2674, 4013, 4143, 2278, 2092, 19892, 21823, 2078, 2560, 4228, 3371, 3048, 4190, 2812, 3048, 3564, 4624, 3242, 17901, 5549, 8156, 2672, 3046, 21006, 2740, 3771, 4522, 10468, 2272, 2093, 8875, 4340, 2828, 29221, 2828, 2215, 2052, 3497, 2224, 2445, 2828, 10699, 3853, 2054, 17048, 2187, 2518, 5549, 8156, 18135, 5262, 2678, 2208, 2204, 2028, 3602, 2204, 2028, 5399, 20714, 3294, 7694, 2331, 2445, 21934, 6203, 5440, 2678, 2208, 3652, 2783, 5440, 2678, 2208, 4658, 3544, 2397, 6517, 2619, 3071, 3524, 2245, 7023, 2204, 2518, 24188, 4509, 2051, 22560, 7065, 2884, 2306, 5110, 2088, 6168, 2051, 2147, 2378, 5959, 2051, 4737, 2111, 2467, 2105, 10930, 3203, 19394, 5649, 6180, 2092, 4931, 2364, 2591, 13307, 12202, 2444, 4512, 2130, 12064, 2135, 16342, 2855, 2428, 10667, 2112, 7917, 11689, 5942, 2131, 2152, 16125, 25043, 4521, 9409, 10199, 8261, 2015, 16125, 9530, 14028, 2075, 2242, 7789, 2628, 21881, 3610, 7917, 2116, 6251, 2071, 2228, 7917, 3666, 3185, 3420, 24654, 3401, 7917, 2740, 2465, 4415, 4036, 2498, 8152, 3778, 7917, 2878, 3677, 3114, 2048, 3336, 8448, 2187, 2157, 14163, 12680, 2075, 7813, 2690, 2478, 2668, 2048, 5430, 2386, 9708, 2651, 6745, 6230, 4351, 5430, 9708, 2813, 2156, 20421, 2088, 2554, 3071, 4150, 23569, 27605, 3367, 3063, 3063, 4009, 2801, 4175, 5716, 2242, 2066, 8085, 6160, 8957, 4635, 2711, 20164, 2969, 19593, 12731, 2480, 18568, 8085, 3063, 2066, 7098, 7917, 2635, 2282, 2793, 8038, 10657, 4553, 3745, 20997, 7917, 2172, 8505, 2075, 24665, 25438, 2989, 2785, 4040, 15624, 6289, 2232, 2214, 2152, 2082, 2189, 2657, 2287, 3478, 2270, 4092, 2465, 2095, 3283, 4066, 4342, 2071, 2488, 2597, 2502, 2112, 4945, 2058, 18570, 2066, 2711, 5177, 3012, 4484, 2126, 2693, 7573, 2181, 2707, 2047, 2166]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dJjNPkcJpT-y","colab_type":"text"},"source":["Next we will add the extracted features to the end of each tokenized user posts"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DiXixH3WvvtM","colab":{}},"source":["waste=[post.append(data['words_per_comment'][i]) for i,post in enumerate(tokenized_posts)]\n","waste=[post.append(data['question_per_comment'][i]) for i,post in enumerate(tokenized_posts)]\n","waste=[post.append(data['excl_per_comment'][i]) for i,post in enumerate(tokenized_posts)]\n","waste=[post.append(data['ellipsis_per_comment'][i]) for i,post in enumerate(tokenized_posts)]\n","waste=[post.append(data['@_per_comment'][i]) for i,post in enumerate(tokenized_posts)]\n","waste=[post.append(data['#_per_comment'][i]) for i,post in enumerate(tokenized_posts)]\n","waste=[post.append(data['emojis_per_comment'][i]) for i,post in enumerate(tokenized_posts)]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KWRAswcOw6LN","outputId":"21cc4ebe-9b16-4add-e3e1-be676e12d52b","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1587573012938,"user_tz":-60,"elapsed":1049,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["print(tokenized_posts[0])"],"execution_count":25,"outputs":[{"output_type":"stream","text":["[2617, 2998, 13013, 2121, 2327, 2702, 2377, 26418, 2166, 5278, 3325, 2166, 9377, 2651, 2089, 2566, 2278, 3325, 10047, 16862, 2063, 2197, 2518, 2767, 6866, 9130, 16873, 5920, 2279, 2154, 2717, 3521, 7592, 3374, 2963, 12893, 3019, 3276, 15401, 2051, 2296, 2617, 4598, 3046, 3275, 2524, 2051, 2051, 3930, 6160, 4933, 2208, 2275, 2674, 4013, 4143, 2278, 2092, 19892, 21823, 2078, 2560, 4228, 3371, 3048, 4190, 2812, 3048, 3564, 4624, 3242, 17901, 5549, 8156, 2672, 3046, 21006, 2740, 3771, 4522, 10468, 2272, 2093, 8875, 4340, 2828, 29221, 2828, 2215, 2052, 3497, 2224, 2445, 2828, 10699, 3853, 2054, 17048, 2187, 2518, 5549, 8156, 18135, 5262, 2678, 2208, 2204, 2028, 3602, 2204, 2028, 5399, 20714, 3294, 7694, 2331, 2445, 21934, 6203, 5440, 2678, 2208, 3652, 2783, 5440, 2678, 2208, 4658, 3544, 2397, 6517, 2619, 3071, 3524, 2245, 7023, 2204, 2518, 24188, 4509, 2051, 22560, 7065, 2884, 2306, 5110, 2088, 6168, 2051, 2147, 2378, 5959, 2051, 4737, 2111, 2467, 2105, 10930, 3203, 19394, 5649, 6180, 2092, 4931, 2364, 2591, 13307, 12202, 2444, 4512, 2130, 12064, 2135, 16342, 2855, 2428, 10667, 2112, 7917, 11689, 5942, 2131, 2152, 16125, 25043, 4521, 9409, 10199, 8261, 2015, 16125, 9530, 14028, 2075, 2242, 7789, 2628, 21881, 3610, 7917, 2116, 6251, 2071, 2228, 7917, 3666, 3185, 3420, 24654, 3401, 7917, 2740, 2465, 4415, 4036, 2498, 8152, 3778, 7917, 2878, 3677, 3114, 2048, 3336, 8448, 2187, 2157, 14163, 12680, 2075, 7813, 2690, 2478, 2668, 2048, 5430, 2386, 9708, 2651, 6745, 6230, 4351, 5430, 9708, 2813, 2156, 20421, 2088, 2554, 3071, 4150, 23569, 27605, 3367, 3063, 3063, 4009, 2801, 4175, 5716, 2242, 2066, 8085, 6160, 8957, 4635, 2711, 20164, 2969, 19593, 12731, 2480, 18568, 8085, 3063, 2066, 7098, 7917, 2635, 2282, 2793, 8038, 10657, 4553, 3745, 20997, 7917, 2172, 8505, 2075, 24665, 25438, 2989, 2785, 4040, 15624, 6289, 2232, 2214, 2152, 2082, 2189, 2657, 2287, 3478, 2270, 4092, 2465, 2095, 3283, 4066, 4342, 2071, 2488, 2597, 2502, 2112, 4945, 2058, 18570, 2066, 2711, 5177, 3012, 4484, 2126, 2693, 7573, 2181, 2707, 2047, 2166, 11.12, 0.36, 0.06, 0.3, 0.0, 0.0, 0.0]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N18SABGTpT-7","colab_type":"text"},"source":["### ***Prerparing Data For Training***\n","The posts in our dataset have varying lengths. Some posts are very small while others are very long. To train the model, the input sentences should be of equal length. To create sentences of equal length, one way is to pad the shorter sentences by 0s. However, this can result in a sparse matrix contain large number of 0s. The other way is to pad sentences within each batch. Since we will be training the model in batches, we can pad the sentences within the training batch locally depending upon the length of the longest sentence. To do so, we first need to find the length of each sentence.\n","\n","The following script creates a list of lists where each sublist contains tokenized user's posts, the label of the posts and the length of the posts:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Fl-jLIjbuBAB","colab":{}},"source":["posts_with_len = [[post, Y[i], len(post)]\n","                 for i, post in enumerate(tokenized_posts)]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6ulAMpE4pT-_","colab_type":"text"},"source":["The following script shuffles the data randomly:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LRXd80SguLXh","colab":{}},"source":["random.shuffle(posts_with_len)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Gh_bUCEv3FEl","outputId":"6459977a-5ca7-49c1-cd37-88c4f0395771","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1587573023861,"user_tz":-60,"elapsed":641,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["print(posts_with_len[0])"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[[2941, 3047, 4067, 7098, 2295, 7619, 6904, 15088, 6866, 3041, 11689, 4033, 2102, 2464, 3087, 5254, 2295, 2428, 2204, 3689, 2019, 11631, 5162, 5522, 10194, 1051, 12352, 4328, 12849, 9527, 2080, 2572, 2063, 24924, 4038, 3689, 23066, 6499, 2226, 9004, 6583, 22827, 29147, 2080, 11895, 3217, 3676, 3683, 2941, 2036, 4083, 2653, 7098, 5121, 2524, 3849, 2295, 4083, 3811, 16755, 3225, 22827, 2050, 2568, 2478, 2529, 2887, 5791, 12943, 28199, 2146, 4676, 3228, 4752, 2111, 2342, 2344, 5959, 2166, 2986, 2469, 2553, 3722, 2109, 5223, 5791, 2498, 3308, 2204, 3266, 2272, 2744, 2092, 2196, 2245, 3037, 5995, 2590, 2126, 2488, 2500, 4792, 2367, 2903, 12157, 9398, 3067, 9544, 4512, 2177, 2485, 2767, 6684, 26616, 6594, 2672, 2428, 2066, 3233, 11281, 3116, 3507, 5588, 4011, 4678, 4658, 7910, 2213, 2812, 6160, 7098, 6289, 2156, 5580, 2963, 4283, 8430, 2092, 2228, 25652, 2296, 2828, 6082, 6211, 2500, 2092, 2036, 2191, 10427, 3432, 2183, 2111, 7302, 2518, 2066, 1999, 15549, 2890, 5448, 2242, 2467, 2507, 7481, 2245, 17950, 2404, 4283, 7098, 2467, 4567, 4768, 14038, 22247, 12039, 4385, 2590, 3961, 2491, 3571, 2109, 2228, 2627, 5121, 2052, 3271, 2113, 5204, 2215, 2828, 2360, 2272, 2744, 2748, 2296, 6707, 2191, 7438, 3325, 3432, 2655, 26726, 27911, 4416, 4553, 6707, 9145, 2524, 3066, 3221, 3122, 2158, 2116, 2518, 2626, 24501, 21149, 2094, 6975, 2583, 4906, 3110, 2367, 3241, 2242, 3308, 2318, 3492, 2204, 5064, 2439, 3037, 2672, 8576, 2034, 2161, 2196, 2371, 4792, 3926, 10086, 2288, 4522, 2600, 2217, 2066, 19013, 2028, 6866, 5962, 2132, 9864, 10086, 2265, 5791, 8837, 3689, 2186, 2521, 2304, 8562, 8478, 16718, 2265, 11506, 6904, 2615, 2792, 2145, 2972, 2381, 2092, 3322, 2443, 3384, 17345, 2695, 13076, 8837, 6907, 8554, 3132, 3601, 3013, 5724, 2748, 2903, 3100, 2126, 2228, 2052, 2191, 2166, 6082, 2111, 3970, 2560, 2241, 3325, 2144, 2318, 11922, 2156, 4283, 2132, 16755, 9875, 4168, 11721, 3102, 7691, 13638, 2178, 11547, 2283, 12364, 3969, 2295, 2197, 2028, 2453, 2514, 2978, 6760, 2208, 6789, 2792, 2209, 3398, 2092, 3720, 2404, 6907, 2066, 2028, 2595, 6180, 2828, 5009, 3442, 17364, 3388, 9866, 12192, 2179, 5023, 16371, 6651, 5041, 6909, 3262, 9544, 3689, 14704, 2166, 4933, 3579, 2839, 8837, 12849, 2669, 11631, 2050, 9152, 4213, 17453, 3376, 7284, 2464, 2521, 3492, 3835, 2466, 2293, 2058, 10526, 3371, 2707, 4542, 5221, 2189, 18015, 2666, 2113, 2467, 4699, 3276, 2367, 6180, 2828, 6907, 2189, 9544, 2179, 3720, 2667, 11147, 4937, 17122, 5525, 2092, 7893, 2094, 2156, 2081, 6707, 11689, 4067, 7302, 2191, 2469, 2488, 2279, 2051, 6289, 2232, 2217, 5870, 2052, 3437, 2742, 2729, 13736, 2150, 2146, 2354, 2711, 19366, 2302, 2342, 2113, 2498, 2842, 2052, 19957, 2404, 5993, 3294, 2424, 6517, 3305, 3085, 2051, 2113, 2524, 6669, 7481, 2748, 5791, 10107, 2196, 3198, 3437, 2467, 2092, 2903, 2202, 8424, 3198, 2393, 2130, 3571, 13893, 2071, 26881, 2052, 2978, 8401, 2131, 2746, 5121, 2228, 2111, 12774, 11850, 2969, 3037, 29245, 3993, 2529, 23997, 2036, 3811, 18002, 2126, 2535, 29176, 7714, 2428, 2729, 4909, 2393, 7481, 2411, 2424, 2524, 3198, 2467, 2371, 2203, 2131, 2242, 2560, 4468, 3110, 2919, 9643, 2135, 5875, 9826, 2215, 2113, 2131, 13675, 23496, 2812, 4439, 2242, 13675, 23496, 2100, 4025, 2066, 2145, 5158, 2823, 2428, 3109, 6260, 2196, 5599, 3087, 2295, 2551, 15887, 10427, 3067, 3214, 2825, 17111, 3305, 11178, 13399, 3672, 2272, 5094, 2111, 2442, 17087, 2130, 5627, 2175, 13675, 23496, 5667, 2928, 2245, 3599, 2215, 2360, 4821, 2903, 2919, 2518, 2028, 2442, 4553, 2729, 2130, 3535, 2729, 2500, 2228, 2748, 2553, 2228, 2393, 2113, 2552, 2514, 2204, 2052, 2145, 2393, 2113, 27917, 4847, 2111, 2649, 2052, 2196, 3160, 29536, 10719, 2393, 2500, 7481, 2228, 2729, 2172, 2194, 13593, 2498, 5121, 4067, 2126, 5468, 2204, 2393, 2619, 2514, 2204, 2518, 2113, 2514, 2204, 3198, 3160, 2393, 2092, 2428, 3374, 8501, 12422, 10785, 2092, 2553, 5094, 2767, 2742, 8074, 2505, 2842, 2709, 2191, 2514, 2204, 2066, 2056, 2197, 6251, 2667, 25174, 2111, 2404, 2500, 2028, 2428, 2228, 2228, 3087, 2191, 3601, 2903, 6719, 2681, 2188, 2302, 5746, 2447, 2295, 2823, 2131, 26275, 2189, 4510, 3328, 3371, 4416, 3599, 5630, 3800, 3701, 2048, 2518, 15847, 2066, 5114, 3325, 2296, 2309, 2154, 24447, 5128, 3947, 2488, 4283, 12832, 2298, 2157, 8975, 6569, 3560, 16755, 2242, 8038, 18428, 10848, 7367, 4509, 4609, 10958, 8569, 3683, 4168, 11333, 24532, 13340, 4674, 20868, 2226, 2761, 2422, 5993, 2672, 6296, 27724, 2075, 3689, 27.78, 0.12, 0.1, 0.56, 0.0, 0.0, 0.0], 0, 725]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XhwS1m3_pT_H","colab_type":"text"},"source":["Once the data is shuffled, we will sort the data by the length of the posts. To do so, we will use the sort() function of the list and will tell it that we want to sort the list with respect to the third item in the sublist i.e. the length of the posts."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"su3uXlauuec5","colab":{}},"source":["posts_with_len.sort(key=lambda x: x[2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fAVsjtBD3LfV","outputId":"72203f1c-6049-46ef-ddf5-f83ae40196e2","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1587573028952,"user_tz":-60,"elapsed":1753,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["print(posts_with_len[1])"],"execution_count":30,"outputs":[{"output_type":"stream","text":["[[5931, 3119, 2051, 12383, 6151, 8586, 14097, 1.02, 1.82, 0.0, 0.0, 0.0, 0.0, 0.0], 1, 14]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FveiulpopT_O","colab_type":"text"},"source":["Once the posts are sorted by length, we can remove the length attribute . Execute the following script to do so:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"X6z_UrSLuhKx","colab":{}},"source":["sorted_posts_labels = [(post_lab[0], post_lab[1]) for post_lab in posts_with_len]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Bu2lTt-G3Txk","outputId":"2d292a6c-3d64-4d25-a153-694fc07f424a","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1587573032068,"user_tz":-60,"elapsed":638,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["print(sorted_posts_labels[1])"],"execution_count":32,"outputs":[{"output_type":"stream","text":["([5931, 3119, 2051, 12383, 6151, 8586, 14097, 1.02, 1.82, 0.0, 0.0, 0.0, 0.0, 0.0], 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tb1eOYP5pT_V","colab_type":"text"},"source":["Once the posts are sorted we will convert the dataset so that it can be used to train TensorFlow 2.0 models. Run the following code to convert the sorted dataset into a TensorFlow 2.0-compliant input dataset shape."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"t3qOjy-0vU77","colab":{}},"source":["processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_posts_labels, output_types=(tf.int32, tf.int32))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PNcK4LMHpT_Z","colab_type":"text"},"source":["Finally, we can now pad our dataset for each batch. The batch size we are going to use is 32 which means that after processing the posts of 32 users, the weights of the neural network will be updated. To pad the posts locally with respect to batches, execute the following:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"896uIlG7vXtS","colab":{}},"source":["BATCH_SIZE = 32\n","batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fRMGmRyzpT_g","colab_type":"text"},"source":["Let's print the first batch and see how padding has been applied to it:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"71UIW85y3YZS","outputId":"c9d59073-294b-4fd2-9e4e-67589a49e8e4","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1587573040697,"user_tz":-60,"elapsed":1041,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["next(iter(batched_dataset))"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(32, 131), dtype=int32, numpy=\n"," array([[    0,     0,     0, ...,     0,     0,     0],\n","        [ 5931,  3119,  2051, ...,     0,     0,     0],\n","        [ 2559,  2830,  3116, ...,     0,     0,     0],\n","        ...,\n","        [21505,  3944,  3626, ...,     0,     0,     0],\n","        [ 2525,  2411,  2224, ...,     0,     0,     0],\n","        [ 2296,  2309,  3412, ...,     0,     0,     0]], dtype=int32)>,\n"," <tf.Tensor: shape=(32,), dtype=int32, numpy=\n"," array([0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n","        0, 0, 1, 0, 1, 0, 0, 1, 0, 0], dtype=int32)>)"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"5TCyaFktpT_k","colab_type":"text"},"source":["The above output shows the first and last few padded user's posts. From the last users, you can see that the total number of words in the largest sentence were 131. Therefore, in the first users the 0s are added at the end of the sentences so that their total length is also 131. The padding for the next batch will be different depending upon the size of the largest sentence in the batch.\n","\n"," *ps.For the last users the 0 are the features added not the padding zeros\n","\n","Once we have applied padding to our dataset, the next step is to divide the dataset into test and training sets. We can do that with the help of following code:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qzlNks6H3kay","colab":{}},"source":["TOTAL_BATCHES = math.ceil(len(sorted_posts_labels) / BATCH_SIZE)\n","TEST_BATCHES = TOTAL_BATCHES // 10\n","VALID_BATCHES = TOTAL_BATCHES // 10\n","batched_dataset.shuffle(TOTAL_BATCHES)\n","test_data = batched_dataset.take(TEST_BATCHES)\n","train_data = batched_dataset.skip(TEST_BATCHES)\n","valid_data = batched_dataset.take(VALID_BATCHES)\n","train_data = batched_dataset.skip(VALID_BATCHES)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fvK32e4ppT_o","colab_type":"text"},"source":["In the code above we first find the total number of batches by dividing the total records by 32. Next, 10% of the data is left aside for testing and another 10% for validation. To do so, we use the take() method of batched_dataset() object to store 10% of the data in the test_data variable. The remaining data is stored in the train_data object for training using the skip() method.\n","The same is done for the validation data.\n","\n","The dataset has been prepared and now we are ready to create our text classification model."]},{"cell_type":"markdown","metadata":{"id":"q1wq8_HApT_p","colab_type":"text"},"source":["### ***Creating the Model***\n","Now we are all set to create our model. To do so, we will create a class named TEXT_MODEL that inherits from the tf.keras.Model class. Inside the class we will define our model layers. Our model will consist of three convolutional neural network layers. You can use LSTM layers instead and can also increase or decrease the number of layers.\n","\n","Let's now create out model class:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R3SZdCh830ki","colab":{}},"source":["class TEXT_MODEL(tf.keras.Model):\n","    \n","    def __init__(self,\n","                 vocabulary_size,\n","                 embedding_dimensions=128,\n","                 cnn_filters=50,\n","                 dnn_units=512,\n","                 model_output_classes=2,\n","                 dropout_rate=0.1,\n","                 training=False,\n","                 name=\"text_model\"):\n","        super(TEXT_MODEL, self).__init__(name=name)\n","        \n","        self.embedding = layers.Embedding(vocabulary_size,\n","                                          embedding_dimensions)\n","        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n","                                        kernel_size=2,\n","                                        padding=\"valid\",\n","                                        activation=\"relu\")\n","        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n","                                        kernel_size=3,\n","                                        padding=\"valid\",\n","                                        activation=\"relu\")\n","        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n","                                        kernel_size=4,\n","                                        padding=\"valid\",\n","                                        activation=\"relu\")\n","        self.pool = layers.GlobalMaxPool1D()\n","        \n","        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n","        self.dropout = layers.Dropout(rate=dropout_rate)\n","        if model_output_classes == 2:\n","            self.last_dense = layers.Dense(units=1,\n","                                           activation=\"sigmoid\")\n","        else:\n","            self.last_dense = layers.Dense(units=model_output_classes,\n","                                           activation=\"softmax\")\n","    \n","    def call(self, inputs, training):\n","        l = self.embedding(inputs)\n","        l_1 = self.cnn_layer1(l) \n","        l_1 = self.pool(l_1) \n","        l_2 = self.cnn_layer2(l) \n","        l_2 = self.pool(l_2)\n","        l_3 = self.cnn_layer3(l)\n","        l_3 = self.pool(l_3) \n","        \n","        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n","        concatenated = self.dense_1(concatenated)\n","        concatenated = self.dropout(concatenated, training)\n","        model_output = self.last_dense(concatenated)\n","        \n","        return model_output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b7PLnCmspT_t","colab_type":"text"},"source":["The above script is pretty straightforward. In the constructor of the class, we initialze some attributes with default values. These values will be replaced later on by the values passed when the object of the TEXT_MODEL class is created.\n","\n","Next, three convolutional neural network layers have been initialized with the kernel or filter values of 2, 3, and 4, respectively. Again, you can change the filter sizes if you want.\n","\n","Next, inside the call() function, global max pooling is applied to the output of each of the convolutional neural network layer. Finally, the three convolutional neural network layers are concatenated together and their output is fed to the first densely connected neural network. The second densely connected neural network is used to predict the output sentiment since it only contains 2 classes. In case you have more classes in the output, you can updated the output_classes variable accordingly.\n","\n","Let's now define the values for the hyper parameters of our model."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0DleGxDO32xK","colab":{}},"source":["VOCAB_LENGTH = len(tokenizer.vocab)\n","EMB_DIM = 200\n","CNN_FILTERS = 100\n","DNN_UNITS = 256\n","OUTPUT_CLASSES = 2\n","\n","DROPOUT_RATE = 0.2\n","\n","NB_EPOCHS = 5"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tcGF4b_ypT_w","colab_type":"text"},"source":["Next, we need to create an object of the TEXT_MODEL class and pass the hyper paramters values that we defined in the last step to the constructor of the TEXT_MODEL class."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"_VvhYlyk34oi","colab":{}},"source":["text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n","                        embedding_dimensions=EMB_DIM,\n","                        cnn_filters=CNN_FILTERS,\n","                        dnn_units=DNN_UNITS,\n","                        model_output_classes=OUTPUT_CLASSES,\n","                        dropout_rate=DROPOUT_RATE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x999oYYwpT_2","colab_type":"text"},"source":["Before we can actually train the model we need to compile it. The following script compiles the model:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iTM40f4136LD","colab":{}},"source":["if OUTPUT_CLASSES == 2:\n","    text_model.compile(loss=\"binary_crossentropy\",\n","                       optimizer=\"adam\",\n","                       metrics=[\"accuracy\"])\n","else:\n","    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n","                       optimizer=\"adam\",\n","                       metrics=[\"sparse_categorical_accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGu1wjfMpT_7","colab_type":"text"},"source":["Finally to train our model, we can use the fit method of the model class."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B4S-Mhgv39Ap","outputId":"538bfbc6-4581-4304-ae7c-20bc4c222fb6","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1587573198100,"user_tz":-60,"elapsed":114394,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["text_model.fit(train_data, validation_data=valid_data, epochs=NB_EPOCHS)"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","245/245 [==============================] - 29s 118ms/step - loss: 0.3930 - accuracy: 0.8683 - val_loss: 0.4832 - val_accuracy: 0.8044\n","Epoch 2/5\n","245/245 [==============================] - 19s 78ms/step - loss: 0.2988 - accuracy: 0.8767 - val_loss: 0.5661 - val_accuracy: 0.6806\n","Epoch 3/5\n","245/245 [==============================] - 19s 79ms/step - loss: 0.1043 - accuracy: 0.9639 - val_loss: 0.7171 - val_accuracy: 0.7315\n","Epoch 4/5\n","245/245 [==============================] - 19s 78ms/step - loss: 0.0072 - accuracy: 0.9995 - val_loss: 0.8331 - val_accuracy: 0.7315\n","Epoch 5/5\n","245/245 [==============================] - 19s 78ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.9412 - val_accuracy: 0.6748\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7f53febaa630>"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"Gv1Kk7kDpT__","colab_type":"text"},"source":["Let's now evaluate our model's performance on the test set:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JL7yyYNZ4Sa1","outputId":"f2609ba8-1da8-4571-d57f-78ff62539fba","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1587573206383,"user_tz":-60,"elapsed":1042,"user":{"displayName":"Mohamed Amine","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhFcZr-gX-_NqT7LLvvj_nA-vtLNVQuYTjvPBE0=s64","userId":"18183562247018212987"}}},"source":["results = text_model.evaluate(test_data)\n","print(results)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["27/27 [==============================] - 0s 10ms/step - loss: 0.9412 - accuracy: 0.6748\n","[0.9411980509757996, 0.6747685074806213]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rIgGa7dqvKjK","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}